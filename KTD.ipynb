{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "1794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jens/miniconda3/lib/python3.6/site-packages/torch/autograd/__init__.py:93: UserWarning: retain_variables option is deprecated and will be removed in 0.3. Use retain_graph instead.\n",
      "  warnings.warn(\"retain_variables option is deprecated and will be removed in 0.3. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 0 finished after 10 steps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jens/miniconda3/lib/python3.6/site-packages/torch/autograd/__init__.py:93: UserWarning: retain_variables option is deprecated and will be removed in 0.3. Use retain_graph instead.\n",
      "  warnings.warn(\"retain_variables option is deprecated and will be removed in 0.3. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 1 finished after 17 steps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 2 finished after 20 steps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 3 finished after 14 steps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 4 finished after 19 steps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d1f5873030ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Complete'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-d1f5873030ea>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(e, environment)\u001b[0m\n\u001b[1;32m    122\u001b[0m                      FloatTensor([reward])))\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-d1f5873030ea>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     \u001b[0mkalman\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Solution of Open AI gym environment \"Cartpole-v0\" (https://gym.openai.com/envs/CartPole-v0) using DQN and Pytorch.\n",
    "# It is is slightly modified version of Pytorch DQN tutorial from\n",
    "# http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html.\n",
    "# The main difference is that it does not take rendered screen as input but it simply uses observation values from the \\\n",
    "# environment.\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# hyper parameters\n",
    "EPISODES = 400  # number of episodes\n",
    "EPS_START = 0.9  # e-greedy threshold start value\n",
    "EPS_END = 0.05  # e-greedy threshold end value\n",
    "EPS_DECAY = 100  # e-greedy threshold decay\n",
    "GAMMA = 0.9  # Q-learning discount factor\n",
    "LR = 1  # NN optimizer learning rate\n",
    "HIDDEN_LAYER = 256  # NN hidden layer size\n",
    "BATCH_SIZE = 64  # Q-learning batch size\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def latest(self):\n",
    "        return self.memory[-1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, HIDDEN_LAYER)\n",
    "        self.l2 = nn.Linear(HIDDEN_LAYER, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "\"\"\"\n",
    "State space model:\n",
    "thate_t+1 = theta_t + Q\n",
    "r_t = Q(s_t,a_t;thetaPred_t) -gamma*max_a Q(s_t+1,a;thetaPred_t) + R\n",
    "\"\"\"\n",
    "class Kalman():\n",
    "    def __init__(self, params, Q0, R0, P0=1):\n",
    "        self.Q = Q0*np.diag(np.ones(params))\n",
    "        self.R = R0\n",
    "        self.P = P0*np.diag(np.ones(params))\n",
    "        self.K = np.zeros(params)\n",
    "    def updateP(self,newP):\n",
    "        self.P -= newP\n",
    "    \n",
    "env = gym.make('CartPole-v0')\n",
    "env = wrappers.Monitor(env, './tmp/cartpole-v0-1', force=True)\n",
    "\n",
    "model = Network()\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "memory = ReplayMemory(10000)\n",
    "optimizer = optim.SGD(model.parameters(), LR)\n",
    "steps_done = 0\n",
    "episode_durations = []\n",
    "kalman = Kalman(params,0.01,0.001,1)\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        return model(Variable(state, volatile=True).type(FloatTensor)).data.max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return LongTensor([[random.randrange(2)]])\n",
    "\n",
    "\n",
    "def run_episode(e, environment):\n",
    "    state = environment.reset()\n",
    "    steps = 0\n",
    "    while True:\n",
    "        environment.render()\n",
    "        action = select_action(FloatTensor([state]))\n",
    "        next_state, reward, done, _ = environment.step(action[0, 0])\n",
    "        # negative reward when attempt ends\n",
    "        if done:\n",
    "            reward = -1\n",
    "        #a = model(torch.autograd.Variable(torch.from_numpy(state))\n",
    "        memory.push((FloatTensor([state]),\n",
    "                     action,  # action is already a tensor\n",
    "                     FloatTensor([next_state]),\n",
    "                     FloatTensor([reward])))\n",
    "\n",
    "        learn()\n",
    "\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "        if done:\n",
    "            print(\"{2} Episode {0} finished after {1} steps\"\n",
    "                  .format(e, steps, '\\033[92m' if steps >= 195 else '\\033[99m'))\n",
    "            episode_durations.append(steps)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "def learn():\n",
    "    #if len(memory) < BATCH_SIZE:\n",
    "    #    return\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    \"\"\"transitions = memory.sample(BATCH_SIZE)\n",
    "    batch_state, batch_action, batch_next_state, batch_reward = zip(*transitions)\n",
    "    batch_state = Variable(torch.cat(batch_state))\n",
    "    batch_action = Variable(torch.cat(batch_action))\n",
    "    batch_reward = Variable(torch.cat(batch_reward))\n",
    "    batch_next_state = Variable(torch.cat(batch_next_state))\n",
    "\"\"\"\n",
    "    \n",
    "    current = memory.latest()\n",
    "    curr_state, curr_action, curr_next_state, curr_reward = current\n",
    "    curr_state = Variable(torch.cat(curr_state))\n",
    "    curr_action = Variable(torch.cat(curr_action))\n",
    "    curr_reward = Variable(curr_reward)\n",
    "    curr_next_state = Variable(torch.cat(curr_next_state))\n",
    "\n",
    "    # current Q values are estimated by NN for all actions\n",
    "    # current_q_values = model(batch_state).gather(1, batch_action)\n",
    "    current_q_values = model(curr_state).gather(0, curr_action)\n",
    "    # expected Q values are estimated from actions which gives maximum Q value\n",
    "    #max_next_q_values = model(batch_next_state).detach().max(1)[0]\n",
    "    #expected_q_values = batch_reward + (GAMMA * max_next_q_values)\n",
    "    max_next_q_values = model(curr_next_state).detach().max()\n",
    "    expected_q_values = curr_reward + (GAMMA * max_next_q_values)\n",
    "\n",
    "    d = current_q_values\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    H = np.zeros((1,params))\n",
    "    d.backward(torch.FloatTensor([1]),retain_variables=True)\n",
    "    i = 0\n",
    "    sizes = [i]\n",
    "    for name, parameter in model.named_parameters():\n",
    "        temp = parameter.grad.view(-1).data.numpy()\n",
    "        H[0,i:i+temp.shape[0]] = temp\n",
    "        i += temp.shape[0]\n",
    "        sizes.append(i)\n",
    "    \n",
    "    Ppred = kalman.P+kalman.Q\n",
    "    y = expected_q_values-current_q_values #Innovation\n",
    "\n",
    "    S = np.matmul(np.matmul(H,Ppred),np.transpose(H))+kalman.R\n",
    "    K = (np.matmul(Ppred,np.transpose(H)))/S\n",
    "    theta_new = K*(y.data.numpy())\n",
    "\n",
    "    i = 0\n",
    "    for f in model.parameters():\n",
    "        b = torch.from_numpy(theta_new[sizes[i]:sizes[i+1]]).type(FloatTensor)\n",
    "        b = b.view(f.grad.size())\n",
    "        f.data += b\n",
    "        i+=1\n",
    "        \n",
    "    P = np.matmul(K*S,np.transpose(K))\n",
    "    kalman.updateP(P)\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.FloatTensor(episode_durations)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    run_episode(e, env)\n",
    "\n",
    "print('Complete')\n",
    "env.render(close=True)\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
